{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202f4826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomato  파일 길이 :  1000\n",
      "tomato  :  project_fruit/train/tomato\\0.jpg\n",
      "tomato  :  project_fruit/train/tomato\\729.jpg\n",
      "avocado  파일 길이 :  1169\n",
      "avocado  :  project_fruit/train/avocado\\0.jpg\n",
      "avocado  :  project_fruit/train/avocado\\577.jpg\n",
      "strawberry  파일 길이 :  1002\n",
      "strawberry  :  project_fruit/train/strawberry\\0.jpg\n",
      "strawberry  :  project_fruit/train/strawberry\\727.jpg\n",
      "zucchini  파일 길이 :  1009\n",
      "zucchini  :  project_fruit/train/zucchini\\0.jpg\n",
      "zucchini  :  project_fruit/train/zucchini\\720.jpg\n",
      "ok 4180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gajig\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "caltech_dir = \"project_fruit/train\"\n",
    "categories = [\"tomato\", \"avocado\", \"strawberry\", \"zucchini\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, cat in enumerate(categories):\n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx] = 1\n",
    "\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.jpg\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "    for i, f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "\n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "\n",
    "        if i % 700 == 0:\n",
    "            print(cat, \" : \", f)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "#1 0 0 0 이면 airplanes\n",
    "#0 1 0 0 이면 buddha 이런식\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "np.save(\"multi_image_data.npy\", xy)\n",
    "\n",
    "print(\"ok\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5299bd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3135, 64, 64, 3)\n",
      "3135\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session =  tf.compat.v1.Session(config=config)\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load('multi_image_data.npy',allow_pickle=True)\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce35508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'],run_eagerly=True)\n",
    "    model_dir = './model'\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    model_path = model_dir + '/multi_img_classification.model'\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f42c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               4194560   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,214,980\n",
      "Trainable params: 4,214,980\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "155af374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 44.4097 - accuracy: 0.3180\n",
      "Epoch 1: val_loss improved from inf to 1.38138, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 20s 198ms/step - loss: 44.4097 - accuracy: 0.3180 - val_loss: 1.3814 - val_accuracy: 0.2660\n",
      "Epoch 2/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1997 - accuracy: 0.4431\n",
      "Epoch 2: val_loss did not improve from 1.38138\n",
      "98/98 [==============================] - 19s 192ms/step - loss: 1.1997 - accuracy: 0.4431 - val_loss: 1.3958 - val_accuracy: 0.2746\n",
      "Epoch 3/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0685 - accuracy: 0.4954\n",
      "Epoch 3: val_loss improved from 1.38138 to 1.26356, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 20s 201ms/step - loss: 1.0685 - accuracy: 0.4954 - val_loss: 1.2636 - val_accuracy: 0.3943\n",
      "Epoch 4/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9948 - accuracy: 0.5104\n",
      "Epoch 4: val_loss improved from 1.26356 to 1.24686, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 20s 204ms/step - loss: 0.9948 - accuracy: 0.5104 - val_loss: 1.2469 - val_accuracy: 0.3933\n",
      "Epoch 5/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9260 - accuracy: 0.5474\n",
      "Epoch 5: val_loss did not improve from 1.24686\n",
      "98/98 [==============================] - 20s 208ms/step - loss: 0.9260 - accuracy: 0.5474 - val_loss: 1.6422 - val_accuracy: 0.2823\n",
      "Epoch 6/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9104 - accuracy: 0.5560\n",
      "Epoch 6: val_loss improved from 1.24686 to 1.06591, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 23s 237ms/step - loss: 0.9104 - accuracy: 0.5560 - val_loss: 1.0659 - val_accuracy: 0.4785\n",
      "Epoch 7/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8515 - accuracy: 0.5764\n",
      "Epoch 7: val_loss improved from 1.06591 to 0.89394, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 27s 278ms/step - loss: 0.8515 - accuracy: 0.5764 - val_loss: 0.8939 - val_accuracy: 0.5282\n",
      "Epoch 8/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8253 - accuracy: 0.5911\n",
      "Epoch 8: val_loss improved from 0.89394 to 0.79399, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 23s 236ms/step - loss: 0.8253 - accuracy: 0.5911 - val_loss: 0.7940 - val_accuracy: 0.5694\n",
      "Epoch 9/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7846 - accuracy: 0.6137\n",
      "Epoch 9: val_loss did not improve from 0.79399\n",
      "98/98 [==============================] - 23s 232ms/step - loss: 0.7846 - accuracy: 0.6137 - val_loss: 0.8052 - val_accuracy: 0.5569\n",
      "Epoch 10/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7603 - accuracy: 0.6265\n",
      "Epoch 10: val_loss did not improve from 0.79399\n",
      "98/98 [==============================] - 21s 218ms/step - loss: 0.7603 - accuracy: 0.6265 - val_loss: 0.9644 - val_accuracy: 0.5196\n",
      "Epoch 11/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7445 - accuracy: 0.6364\n",
      "Epoch 11: val_loss improved from 0.79399 to 0.75795, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 21s 213ms/step - loss: 0.7445 - accuracy: 0.6364 - val_loss: 0.7579 - val_accuracy: 0.5866\n",
      "Epoch 12/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7251 - accuracy: 0.6408\n",
      "Epoch 12: val_loss improved from 0.75795 to 0.74813, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 23s 232ms/step - loss: 0.7251 - accuracy: 0.6408 - val_loss: 0.7481 - val_accuracy: 0.6258\n",
      "Epoch 13/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7076 - accuracy: 0.6552\n",
      "Epoch 13: val_loss did not improve from 0.74813\n",
      "98/98 [==============================] - 19s 192ms/step - loss: 0.7076 - accuracy: 0.6552 - val_loss: 0.7698 - val_accuracy: 0.6172\n",
      "Epoch 14/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.6692\n",
      "Epoch 14: val_loss did not improve from 0.74813\n",
      "98/98 [==============================] - 19s 190ms/step - loss: 0.6893 - accuracy: 0.6692 - val_loss: 0.8097 - val_accuracy: 0.5856\n",
      "Epoch 15/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6854 - accuracy: 0.6702\n",
      "Epoch 15: val_loss improved from 0.74813 to 0.72405, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 20s 208ms/step - loss: 0.6854 - accuracy: 0.6702 - val_loss: 0.7240 - val_accuracy: 0.6526\n",
      "Epoch 16/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6632 - accuracy: 0.6823\n",
      "Epoch 16: val_loss did not improve from 0.72405\n",
      "98/98 [==============================] - 23s 230ms/step - loss: 0.6632 - accuracy: 0.6823 - val_loss: 0.7419 - val_accuracy: 0.6364\n",
      "Epoch 17/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6528 - accuracy: 0.6813\n",
      "Epoch 17: val_loss improved from 0.72405 to 0.70860, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 22s 229ms/step - loss: 0.6528 - accuracy: 0.6813 - val_loss: 0.7086 - val_accuracy: 0.6498\n",
      "Epoch 18/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6175 - accuracy: 0.7120\n",
      "Epoch 18: val_loss did not improve from 0.70860\n",
      "98/98 [==============================] - 20s 205ms/step - loss: 0.6175 - accuracy: 0.7120 - val_loss: 0.7119 - val_accuracy: 0.6421\n",
      "Epoch 19/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6095 - accuracy: 0.7174\n",
      "Epoch 19: val_loss improved from 0.70860 to 0.70410, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 23s 233ms/step - loss: 0.6095 - accuracy: 0.7174 - val_loss: 0.7041 - val_accuracy: 0.6833\n",
      "Epoch 20/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5767 - accuracy: 0.7308\n",
      "Epoch 20: val_loss did not improve from 0.70410\n",
      "98/98 [==============================] - 22s 221ms/step - loss: 0.5767 - accuracy: 0.7308 - val_loss: 0.7103 - val_accuracy: 0.6593\n",
      "Epoch 21/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5737 - accuracy: 0.7397\n",
      "Epoch 21: val_loss improved from 0.70410 to 0.67927, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 22s 229ms/step - loss: 0.5737 - accuracy: 0.7397 - val_loss: 0.6793 - val_accuracy: 0.6976\n",
      "Epoch 22/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5769 - accuracy: 0.7445\n",
      "Epoch 22: val_loss improved from 0.67927 to 0.66109, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 23s 238ms/step - loss: 0.5769 - accuracy: 0.7445 - val_loss: 0.6611 - val_accuracy: 0.7053\n",
      "Epoch 23/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5220 - accuracy: 0.7691\n",
      "Epoch 23: val_loss improved from 0.66109 to 0.65802, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 21s 218ms/step - loss: 0.5220 - accuracy: 0.7691 - val_loss: 0.6580 - val_accuracy: 0.7053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5076 - accuracy: 0.7809\n",
      "Epoch 24: val_loss improved from 0.65802 to 0.64691, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 23s 239ms/step - loss: 0.5076 - accuracy: 0.7809 - val_loss: 0.6469 - val_accuracy: 0.7110\n",
      "Epoch 25/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.7834\n",
      "Epoch 25: val_loss improved from 0.64691 to 0.64235, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 20s 202ms/step - loss: 0.5050 - accuracy: 0.7834 - val_loss: 0.6424 - val_accuracy: 0.7081\n",
      "Epoch 26/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.7793\n",
      "Epoch 26: val_loss improved from 0.64235 to 0.62911, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 20s 200ms/step - loss: 0.5005 - accuracy: 0.7793 - val_loss: 0.6291 - val_accuracy: 0.7263\n",
      "Epoch 27/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4789 - accuracy: 0.7939\n",
      "Epoch 27: val_loss did not improve from 0.62911\n",
      "98/98 [==============================] - 19s 189ms/step - loss: 0.4789 - accuracy: 0.7939 - val_loss: 0.6474 - val_accuracy: 0.7273\n",
      "Epoch 28/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4489 - accuracy: 0.8067\n",
      "Epoch 28: val_loss improved from 0.62911 to 0.61521, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 20s 206ms/step - loss: 0.4489 - accuracy: 0.8067 - val_loss: 0.6152 - val_accuracy: 0.7321\n",
      "Epoch 29/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4348 - accuracy: 0.8131\n",
      "Epoch 29: val_loss did not improve from 0.61521\n",
      "98/98 [==============================] - 20s 206ms/step - loss: 0.4348 - accuracy: 0.8131 - val_loss: 0.6526 - val_accuracy: 0.7359\n",
      "Epoch 30/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4017 - accuracy: 0.8313\n",
      "Epoch 30: val_loss did not improve from 0.61521\n",
      "98/98 [==============================] - 20s 207ms/step - loss: 0.4017 - accuracy: 0.8313 - val_loss: 0.6199 - val_accuracy: 0.7435\n",
      "Epoch 31/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4043 - accuracy: 0.8348\n",
      "Epoch 31: val_loss did not improve from 0.61521\n",
      "98/98 [==============================] - 23s 235ms/step - loss: 0.4043 - accuracy: 0.8348 - val_loss: 0.6363 - val_accuracy: 0.7684\n",
      "Epoch 32/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4287 - accuracy: 0.8230\n",
      "Epoch 32: val_loss did not improve from 0.61521\n",
      "98/98 [==============================] - 20s 204ms/step - loss: 0.4287 - accuracy: 0.8230 - val_loss: 0.6214 - val_accuracy: 0.7455\n",
      "Epoch 33/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3463 - accuracy: 0.8549\n",
      "Epoch 33: val_loss improved from 0.61521 to 0.59911, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 20s 205ms/step - loss: 0.3463 - accuracy: 0.8549 - val_loss: 0.5991 - val_accuracy: 0.7818\n",
      "Epoch 34/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.8501\n",
      "Epoch 34: val_loss did not improve from 0.59911\n",
      "98/98 [==============================] - 20s 209ms/step - loss: 0.3594 - accuracy: 0.8501 - val_loss: 0.6176 - val_accuracy: 0.7617\n",
      "Epoch 35/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3482 - accuracy: 0.8593\n",
      "Epoch 35: val_loss did not improve from 0.59911\n",
      "98/98 [==============================] - 20s 204ms/step - loss: 0.3482 - accuracy: 0.8593 - val_loss: 0.6209 - val_accuracy: 0.7751\n",
      "Epoch 36/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3435 - accuracy: 0.8676\n",
      "Epoch 36: val_loss did not improve from 0.59911\n",
      "98/98 [==============================] - 20s 206ms/step - loss: 0.3435 - accuracy: 0.8676 - val_loss: 0.6614 - val_accuracy: 0.7560\n",
      "Epoch 37/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3389 - accuracy: 0.8606\n",
      "Epoch 37: val_loss did not improve from 0.59911\n",
      "98/98 [==============================] - 22s 224ms/step - loss: 0.3389 - accuracy: 0.8606 - val_loss: 0.6221 - val_accuracy: 0.7895\n",
      "Epoch 38/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3379 - accuracy: 0.8616\n",
      "Epoch 38: val_loss improved from 0.59911 to 0.57644, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "98/98 [==============================] - 21s 214ms/step - loss: 0.3379 - accuracy: 0.8616 - val_loss: 0.5764 - val_accuracy: 0.7856\n",
      "Epoch 39/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2995 - accuracy: 0.8727\n",
      "Epoch 39: val_loss did not improve from 0.57644\n",
      "98/98 [==============================] - 21s 219ms/step - loss: 0.2995 - accuracy: 0.8727 - val_loss: 0.6109 - val_accuracy: 0.7895\n",
      "Epoch 40/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.8874\n",
      "Epoch 40: val_loss did not improve from 0.57644\n",
      "98/98 [==============================] - 19s 190ms/step - loss: 0.2861 - accuracy: 0.8874 - val_loss: 0.6610 - val_accuracy: 0.7675\n",
      "Epoch 41/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3083 - accuracy: 0.8794\n",
      "Epoch 41: val_loss did not improve from 0.57644\n",
      "98/98 [==============================] - 19s 191ms/step - loss: 0.3083 - accuracy: 0.8794 - val_loss: 0.5934 - val_accuracy: 0.7847\n",
      "Epoch 42/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2968 - accuracy: 0.8829\n",
      "Epoch 42: val_loss did not improve from 0.57644\n",
      "98/98 [==============================] - 19s 189ms/step - loss: 0.2968 - accuracy: 0.8829 - val_loss: 0.6345 - val_accuracy: 0.7789\n",
      "Epoch 43/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.8900\n",
      "Epoch 43: val_loss did not improve from 0.57644\n",
      "98/98 [==============================] - 19s 191ms/step - loss: 0.2592 - accuracy: 0.8900 - val_loss: 0.6442 - val_accuracy: 0.7847\n",
      "Epoch 44/50\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2716 - accuracy: 0.8963\n",
      "Epoch 44: val_loss did not improve from 0.57644\n",
      "98/98 [==============================] - 19s 189ms/step - loss: 0.2716 - accuracy: 0.8963 - val_loss: 0.6452 - val_accuracy: 0.7943\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5f7f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 2s 54ms/step - loss: 0.6452 - accuracy: 0.7943\n",
      "정확도 : 0.7943\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4724492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04dbdf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000 0.964 0.000 0.035]\n",
      "1\n",
      "해당 0.jpg이미지는 아보카도으로 추정됩니다.\n",
      "[0.001 0.000 0.000 0.999]\n",
      "3\n",
      "해당 1.jpg이미지는 애호박로 추정됩니다.\n",
      "[0.997 0.000 0.003 0.000]\n",
      "0\n",
      "해당 100.jpg이미지는 토마토로 추정됩니다.\n",
      "[0.000 0.025 0.000 0.974]\n",
      "3\n",
      "해당 110.jpg이미지는 애호박로 추정됩니다.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "caltech_dir = \"project_fruit/test\"\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "filenames = []\n",
    "files = glob.glob(caltech_dir+\"/*.*\")\n",
    "for i, f in enumerate(files):\n",
    "    img = Image.open(f)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((image_w, image_h))\n",
    "    data = np.asarray(img)\n",
    "    filenames.append(f)\n",
    "    X.append(data)\n",
    "    \n",
    "    \n",
    "X = np.array(X)\n",
    "model = load_model('./model/multi_img_classification.model')\n",
    "\n",
    "prediction = model.predict(X[:4])\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "cnt = 0\n",
    "\n",
    "#이 비교는 그냥 파일들이 있으면 해당 파일과 비교. 카테고리와 함께 비교해서 진행하는 것은 _4 파일.\n",
    "for i in prediction:\n",
    "    pre_ans = i.argmax()  # 예측 레이블\n",
    "    print(i)\n",
    "    print(pre_ans)\n",
    "    pre_ans_str = ''\n",
    "    if pre_ans == 0: pre_ans_str = \"토마토\"\n",
    "    elif pre_ans == 1: pre_ans_str = \"아보카도\"\n",
    "    elif pre_ans == 2: pre_ans_str = \"딸기\"\n",
    "    else: pre_ans_str = \"애호박\"\n",
    "    if i[0] >= 0.8 : print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[1] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"으로 추정됩니다.\")\n",
    "    if i[2] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    if i[3] >= 0.8: print(\"해당 \"+filenames[cnt].split(\"\\\\\")[1]+\"이미지는 \"+pre_ans_str+\"로 추정됩니다.\")\n",
    "    cnt += 1\n",
    "    # print(i.argmax()) #얘가 레이블 [1. 0. 0.] 이런식으로 되어 있는 것을 숫자로 바꿔주는 것.\n",
    "    # 즉 얘랑, 나중에 카테고리 데이터 불러와서 카테고리랑 비교를 해서 같으면 맞는거고, 아니면 틀린거로 취급하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe6383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
